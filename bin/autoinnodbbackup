#!/bin/bash

export PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"

#EMAIL_ALERT=""
DIR_BACKUP="/backup"
PARALLEL_INNOBACKUPEX=1
PARALLEL_COMPRESS=8
TAR_REMOVE_FILES=1
SILENT=0
ROTATION=0
MAX_BACKUP_FILES=2

SCRIPT_UPDATE_URL="https://raw.githubusercontent.com/macskas/autoinnodbbackup/master/bin/autoinnodbbackup"

CONFIG_ALLOWED=( "DIR_BACKUP" "EMAIL_ALERT" "MYSQL_USERNAME" "MYSQL_PASSWORD" "PARALLEL_INNOBACKUPEX" "PARALLEL_COMPRESS" "TAR_REMOVE_FILES" "SILENT" "ROTATION" "MAX_BACKUP_FILES" "SCRIPT_UPDATE_URL" )
BINARY_NEED=( "innobackupex" "find" "tar" "gzip" "mail" )
COMPRESS_PROGRAM="gzip"
MYHOST=$(hostname)
DIR_BACKUPEX="$DIR_BACKUP/tmp"
DIR_LATEST="$DIR_BACKUP/latest"
DIR_DAILY="$DIR_BACKUP/daily"
DATE_SUFFIX=$(date +%Y-%m-%d_%Hh%Mm_%a)
LOGFILE="$DIR_BACKUP/backup.$DATE_SUFFIX.log"
FILE_BACKUP="$DIR_DAILY/$MYHOST.$DATE_SUFFIX.tar.gz"
FILE_CONFIG="/etc/autoinnodbbackup/autoinnodbbackup.conf"
UNIQ_FILENAME="sync.lock"

SCRIPT_VERSION="2016022501"

do_help()
{
    echo "autoinnodbbackup $SCRIPT_VERSION, basic innodb backupex script. For easy bacula backups."
    echo "Usage: $0 [option]"
    echo -e "\t-h: show help"
    echo -e "\t-q: silent mode"
    echo -e "\t-u: update from github (master)"
    echo -e "\t-c <config>: read a config file"
    exit 0
}

do_read_config()
{
    if [ ! -f "$FILE_CONFIG" ]; then
	return 0
    fi
    local i=0
    local e=${#CONFIG_ALLOWED[*]}
    local implode_string=""
    for (( i=0; i<e; ++i )); do
    if [ $i -eq 0 ]; then
        implode_string=${CONFIG_ALLOWED[$i]}
    else
        implode_string="$implode_string|${CONFIG_ALLOWED[$i]}"
    fi
    done
    source <(grep -E "^\s*($implode_string)=" $FILE_CONFIG)
}

do_getopt()
{
    local opt=""
    while getopts "c:hqu" opt; do
	case "$opt" in
		c)
		    FILE_CONFIG="$OPTARG"
		;;
		h)
		    do_help
		;;
		q)
		    SILENT=1
		;;
		u)
		    do_update
		    exit 0
		;;
		\?)
		    echo "Option -$OPTARG invalid" >&2
		    do_help
		;;
	esac
    done
}

do_update()
{
    local curl_ok=0
    local wget_ok=0
    local tmp_f=""
    do_check_binary_one "curl" && curl_ok=1
    do_check_binary_one "wget" && wget_ok=1
    do_check_binary_one "mktemp" || do_error "Need mktemp command for updating"
    do_info "do_update" "download new version from $SCRIPT_URL"

    if [ "$curl_ok" -eq 1 ]; then
	tmp_f=$(mktemp)
	[ -f "$tmp_f" ] || do_error "mktemp failed."
	curl -s --connect-timeout 10 -o "$tmp_f" "$SCRIPT_UPDATE_URL"
	if [ $? -ne 0 ]; then
	    rm -f "$tmp_f"
	    do_error "Unable to download new version"
	fi
    elif [ "$wget_ok" -eq 1 ]; then
	tmp_f=$(mktemp)
	[ -f "$tmp_f" ] || do_error "mktemp failed."
	wget -q -T 10 -O "$tmp_f" "$SCRIPT_UPDATE_URL"
	if [ $? -ne 0 ]; then
    	    rm -f "$tmp_f"
    	    do_error "Unable to download new version"
	fi
    else
	do_error "Need wget or curl"
    fi
    
    local tmp_version=$(grep -E '^\s*SCRIPT_VERSION="[0-9]+"' "$tmp_f"|grep -Eo "[0-9]+")
    if [ ${#tmp_version} -lt 5 ]; then
        rm -f "$tmp_f"
        do_error "Unable to find script_version in the new script."
    fi
    
    if [ "$tmp_version" -le "$SCRIPT_VERSION" ]; then
	do_info "do_update" "You have the latest version. No update required."
	rm -f "$tmp_f"
	exit 0
    fi
    do_info "do_update" "Update is required - new version: $tmp_version."
    # sanity check on arg0
    grep -q SCRIPT_VERSION "$0"
    if [ $? -ne 0 ]; then
	rm -f "$tmp_f"
	do_error "Something wrong with argv[0]: ($0)"
    fi
    chmod 755 "$tmp_f" || do_error "Chmod failed on $tmp_f."
    mv "$tmp_f" "$0"
    do_info "do_update" "Updated was successful."
    exit 0
}

generate_uniq_filename()
{
    UNIQ_FILENAME=$(echo "$0"|md5sum 2>&1|awk '{print $1}')
    if [ "${#UNIQ_FILENAME}" -eq 0 ]; then
	UNIQ_FILENAME="sync.lock"
    else
	UNIQ_FILENAME="$UNIQ_FILENAME.lock"
    fi
}

do_error()
{
    local ftime=$(date +%Y-%m-%d" "%H:%M:%S)
    local msg="$ftime ERROR > $@"
    
    echo "$msg" >> $LOGFILE
    echo "$msg" 1>&2
    if [ ${#EMAIL_ALERT} -gt 5 ]; then
	cat "$LOGFILE"|mail -s "InnoDB Backup Failed. ($MYHOST - $msg)" "$EMAIL_ALERT"
    fi
    exit 1
}

do_info()
{
    local ftime=$(date +%Y-%m-%d" "%H:%M:%S)
    local category="$1"
    shift
    local msg="$ftime INFO $category > $@"
    echo "$msg" >> $LOGFILE
    if [ $SILENT -eq 1 ]; then
	return 0
    fi
    echo "$msg"
}

do_check_binary()
{
    local e=${#BINARY_NEED[*]}
    local i=0
    for (( i=0; i<e; i++ )); do
	which ${BINARY_NEED[$i]} >/dev/null
	if [ $? -ne 0 ]; then
	    do_error "Missing binary: ${BINARY_NEED[$i]}"
	fi
    done
    if [ $PARALLEL_COMPRESS -gt 1 ]; then
	which pigz >/dev/null
	if [ $? -eq 0 ]; then
	    COMPRESS_PROGRAM="pigz -p$PARALLEL_COMPRESS"
	fi
    fi
}

do_cleanup_maxfiles()
{
    local offset="$1"
    offset=$(( offset+0 ))
    local fp=""
    if [ "$MAX_BACKUP_FILES" -le 0 ]; then
	do_info "do_cleanup_maxfiles" "Skip maxfiles cleanup"
	return 0
    fi
    local numfiles=$(find "$DIR_DAILY" -type f -name "*.tar.gz"|wc -l)
    local real_maxfiles=$(( MAX_BACKUP_FILES-offset ))
    if [ $numfiles -gt $real_maxfiles ]; then
	local need_files=$(( numfiles-real_maxfiles ))
	if [ $need_files -gt 0 ]; then
	    while read fp; do
		do_info "do_cleanup_maxfiles" "Delete backup $fp"
		rm "$fp"
	    done < <(find "$DIR_DAILY/" -type f -printf "%T@ %h/%f\n"|sort -n|cut -d ' ' -f2-|head -n $need_files)
	fi
    else
	do_info "do_cleanup_maxfiles" "No action required. Files found: $numfiles"
	return 0
    fi
}

do_cleanup()
{
    if [ ! -d "$DIR_BACKUP" ]; then
	do_error "Missing backup directory: $DIR_BACKUP"
    fi
    local fn=""
    if [ -d "$DIR_BACKUPEX" ]; then
	if [ -f "$DIR_BACKUPEX/backup-my.cnf" ]; then
	    do_info "do_cleanup" "rm -rf $DIR_BACKUPEX"
	    rm -rf "$DIR_BACKUPEX"
	    if [ $? -ne 0 ]; then
		do_error "rm -rf $DIR_BACKUPEX failed."
	    fi
	else
	    do_error "This is not a innodbbackupex directory: $DIR_BACKUPEX."
	fi
    fi
}

do_rotation()
{
    do_info "do_rotation" "Running rotation"
    find $DIR_BACKUP -type f -name "$MYHOST.*.tar.gz" -mtime +$ROTATION -delete
    find $DIR_BACKUP -type f -name "backup.*.log" -mtime +$ROTATION -delete
    do_info "do_rotation" "Finished."
}

do_fix_dirs()
{
    if [ ! -d "$DIR_LATEST" ]; then
	do_info "do_fix_dirs" "Create $DIR_LATEST"
	mkdir "$DIR_LATEST" || do_error "Failed to create: $DIR_LATEST"
    fi

    if [ ! -d "$DIR_DAILY" ]; then
	do_info "do_fix_dirs" "Create $DIR_DAILY"
	mkdir "$DIR_DAILY" || do_error "Failed to create: $DIR_DAILY"
    fi
}

do_cleanup_latest()
{
    if [ ! -d "$DIR_LATEST" ]; then
	do_error "Missing latest directory: $DIR_LATEST"
    fi
    do_info "do_cleanup_latest" "Running cleanup"
    find "$DIR_LATEST" -type f -name "*.tar.gz" -delete
    do_info "do_cleanup_latest" "Finished. ($?)"
}

do_latest()
{
    local fpath="$FILE_BACKUP"
    if [ ! -f "$fpath" ]; then
	do_error "Missing source for latest: $fpath"
    fi
    do_cleanup_latest
    local fname=$(basename $fpath)
    if [ ${#fname} -lt 5 ]; then
	do_error "Filename error [$fpath] to [$fname]"
    fi
    do_info "do_latest" "Creating latest"
    cp -al $fpath "$DIR_LATEST/$fname"
    do_info "do_latest" "Finished. ($?)"
}

do_innodb_backup()
{
    do_info "do_innodb_backup" "Running innodbbackupex"
    innobackupex --parallel=$PARALLEL_INNOBACKUPEX --user="$MYSQL_USERNAME" --password="$MYSQL_PASSWORD" "$DIR_BACKUPEX" --no-timestamp >>$LOGFILE 2>&1
    if [ $? -ne 0 ]; then
	do_cleanup
	do_error "innodbbackupex failed #1"
    fi
    innobackupex --parallel=$PARALLEL_INNOBACKUPEX --apply-log --user="$MYSQL_USERNAME" --password="$MYSQL_PASSWORD" "$DIR_BACKUPEX" --no-timestamp >>$LOGFILE 2>&1
    if [ $? -ne 0 ]; then
	do_cleanup
	do_error "innodbbackupex failed #2"
    fi
    do_info "do_innodb_backup" "Finished."
    if [ ! -f "$DIR_BACKUPEX/backup-my.cnf" ]; then
	do_error "Missing file after innodbbackupex: $DIR_BACKUPEX/backup-my.cnf"
    fi
}

do_compressed_backup()
{
    do_info "do_compressed_backup" "Running compress"
    if [ "$TAR_REMOVE_FILES" -eq 1 ]; then
	tar --remove-files -C "$DIR_BACKUPEX/" -cvf - ./ | $COMPRESS_PROGRAM >$FILE_BACKUP 2>>$LOGFILE
    else
	tar -C "$DIR_BACKUPEX/" -cvf - ./ | $COMPRESS_PROGRAM >$FILE_BACKUP 2>>$LOGFILE
    fi
    if [ $? -ne 0 ]; then
	do_error "Compress failed";
    else
	do_info "do_compressed_backup" "Finished."
    fi
}

do_getopt "$@"

do_check_binary
generate_uniq_filename
do_read_config

(
    flock -n -e 200 || do_error "Another backup is already running"
    do_cleanup
    do_fix_dirs
    do_cleanup_maxfiles 1
    do_rotation
    do_innodb_backup
    do_compressed_backup
    do_latest
    do_cleanup
    do_cleanup_maxfiles 0
) 200>/var/lock/$UNIQ_FILENAME

